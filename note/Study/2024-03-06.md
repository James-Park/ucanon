# BERT
BERT (Bidirectional Encoder Representations from Transformers)는 Google이 개발한 사전 훈련(pre-training)된 자연어 처리(NLP) 모델입니다. BERT의 주요 특징은 다음과 같습니다:

1. **양방향 Transformer 인코딩**: BERT는 문장의 모든 단어를 동시에 고려하는 양방향 인코딩을 사용합니다. 이는 문맥을 더 잘 이해하도록 돕습니다.

2. **사전 훈련과 미세 조정**: BERT는 먼저 대량의 텍스트 데이터(예: 위키백과)에 대해 사전 훈련되며, 이후 특정 작업에 대해 미세 조정됩니다. 이 접근 방식은 다양한 NLP 작업에서 높은 성능을 달성하였습니다.

3. **다양한 NLP 작업에 적용 가능**: BERT는 질문 응답, 개체명 인식, 감성 분석 등 다양한 NLP 작업에 적용할 수 있습니다.

BERT는 이러한 특징 덕분에 자연어 처리 분야에서 혁신적인 성과를 이루었습니다.


# 양방향 인코딩
BERT에서 컴퓨터가 문장을 이해하는 방법 중 하나입니다. 

우리가 문장을 읽을 때, '나는 오늘 학교에 갔다'라는 문장에서 '나는', '오늘', '학교에', '갔다'라는 단어들을 모두 함께 고려해서 전체 문장의 의미를 이해하죠. 

BERT도 비슷한 방식으로 문장을 이해합니다. BERT는 문장의 모든 단어를 동시에 보고, 각 단어가 문장에서 어떤 역할을 하는지, 다른 단어들과 어떻게 관련이 있는지를 파악합니다. 이렇게 하면 컴퓨터도 우리처럼 문장의 전체적인 의미를 더 잘 이해할 수 있어요. 

이런 방식을 '양방향 인코딩'이라고 합니다. '양방향'이라는 말은 '두 방향', 즉 문장의 시작부터 끝까지, 그리고 끝에서 시작까지 모두 보겠다는 의미입니다. 이렇게 하면 각 단어가 문장에서 어떤 의미를 가지는지 더 잘 이해할 수 있습니다.

# Transformer Model 이름의 의미
"트랜스포머(Transformer)"라는 이름은 이 모델이 입력 데이터를 "변환(transform)"하는 방식 때문에 붙여진 이름입니다.

트랜스포머 모델은 입력으로 주어진 문장을 내부적으로 다양한 방식으로 변환하며, 이 변환 과정을 통해 문장의 의미를 이해하려고 합니다. 이 변환 과정은 주로 "어텐션(Attention)" 메커니즘을 통해 이루어지며, 이는 각 단어가 문장 내의 다른 단어들과 어떻게 연관되어 있는지를 파악하는 과정입니다.

따라서 이 모델이 문장을 이해하기 위해 내부적으로 데이터를 "변환(transform)"하는 방식을 사용한다는 점에서 "트랜스포머(Transformer)"라는 이름이 붙여진 것입니다.